{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Define All Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \"\"\"Fungsi untuk melakukan load data pada file .tsv maupun .txt\n",
    "    \n",
    "    File yang digunakan yaitu file .tsv maupun file yang mampu dibuka\n",
    "    menggunakan teks editor. Dalam file tersebut terdapat tag pembuka <kalimat>\n",
    "    dan diakhiri tag penutup </kalimat> yang berfungsi sebagai penanda bahwa\n",
    "    entitas tersebut termasuk ke dalam satu kalimat.\n",
    "    \n",
    "    Args:\n",
    "        filename: string dari nama file yang akan diload datanya.\n",
    "        \n",
    "    Return:\n",
    "        list dari kata-kata dan tags dengan index yang menunjukkan posisi kalimat tersebut.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data dan buka sebagai file\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(filename) as file:\n",
    "        contents = file.readlines()\n",
    "\n",
    "    # Hapus karakter \\n yang tidak dibutuhkan\n",
    "    contents = [content.strip() for content in contents]\n",
    "    idx = 0\n",
    "    while idx < len(contents):\n",
    "        word = []\n",
    "        tag = []\n",
    "        # looping sampai menemukan pattern dengan awalan </kalimat\n",
    "        while not contents[idx].startswith('</kalimat'):\n",
    "            # kondisi jika menemukan sebuah data yang tidak memiliki awalan <kalimat\n",
    "            if not contents[idx].startswith('<kalimat'):\n",
    "                temp_word, temp_tag = contents[idx].split(\"\\t\")\n",
    "                word.append(temp_word.lower())\n",
    "                tag.append(temp_tag)\n",
    "            idx += 1\n",
    "        sentences.append(word)\n",
    "        tags.append(tag)\n",
    "        idx += 2\n",
    "        \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(multi_list):\n",
    "    \"\"\"Mengembalikan list multi dimensi ke dalam list satu dimensi.\n",
    "    \n",
    "    Input list yang masuk di proses menggunakan method from_iterable dari\n",
    "    package itertools dan mengembalikan object berupa generator yang iterable.\n",
    "    \n",
    "    Args:\n",
    "        multi_list: list multi dimensi.\n",
    "    \n",
    "    Return:\n",
    "        list satu dimensi yang nantinya digunakan untuk pemrosesan lebih lanjut.\n",
    "    \"\"\"\n",
    "    \n",
    "    return chain.from_iterable(multi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(dataset):\n",
    "    \"\"\"Fungsi untuk membuat bigram dari list.\n",
    "    \n",
    "    Input list berisi string yang nantinya digenerate bigram dan mengembalikan\n",
    "    objek berupa dictionary python.\n",
    "    \n",
    "    Args:\n",
    "        dataset: list satu dimensi.\n",
    "        \n",
    "    Return:\n",
    "        dictionary yang memuat bigram dan banyaknya bigram pada list tersebut.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigrams = []\n",
    "    for i in range(len(dataset)-1):\n",
    "        bigrams.append((dataset[i], dataset[i+1]))\n",
    "    count_bigrams = Counter(bigrams)\n",
    "    \n",
    "    return bigrams, dict(count_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_transition(dict_bigrams):\n",
    "    \"\"\"Fungsi untuk menghitung probabilitas dari dictionary perhitungan banyaknya muncul pasangan tag\n",
    "    \n",
    "    Tag diambil dan dihitung berdasarkan banyak kemunculan dibagi dengan total keseluruhan tag\n",
    "    yang muncul.\n",
    "    \n",
    "    Args:\n",
    "        dict_bigrams: dictionary berisi pasangan tag dan banyaknya kemunculan tag\n",
    "    \n",
    "    Return:\n",
    "        tabel transisi yang berisi probabilitas kemunculan tag dan tag berikutnya\n",
    "    \n",
    "    \"\"\"\n",
    "    total_cnt = float(sum(dict_bigrams.values()))\n",
    "    probs = []\n",
    "    keys = []\n",
    "    \n",
    "    for key, value in dict_bigrams.items():\n",
    "        probs.append(value / total_cnt)\n",
    "        keys.append(key)\n",
    "    \n",
    "    probs_transition = dict(zip(keys,probs))\n",
    "    return probs_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_table(words, tags):\n",
    "    # Membuat representasi tabel emission probability dari HMM\n",
    "    hidden_state = {}\n",
    "    for word in words:\n",
    "        # Jika word belum pernah ditemui maka akan digenerate\n",
    "        # tagset berserta probabilitasnya\n",
    "        if word not in hidden_state.keys():\n",
    "            word_tags = []\n",
    "            for idx, wrd in enumerate(words):\n",
    "                if wrd == word :\n",
    "                    word_tags.append(tags[idx])\n",
    "            # Membuat dictionary berisi semua tagset dari sebauh word\n",
    "            # pada korpus. Berserta jumlah kemunculan setiap tagsetnya\n",
    "            tag_count = {}\n",
    "            for tag in word_tags:\n",
    "                if tag not in tag_count.keys():\n",
    "                    tag_count[tag] = 1\n",
    "                else:\n",
    "                    tag_count[tag] += 1\n",
    "            total = 0\n",
    "            # Mengubah jumlah kemunculan menjadi probabilitas\n",
    "            tag_prob = {}\n",
    "            for count in tag_count.values():\n",
    "                total += count\n",
    "            for tagset in tag_count.keys():\n",
    "                tag_prob[tagset] = tag_count[tagset]/total\n",
    "            hidden_state[word] = tag_prob\n",
    "        else:\n",
    "            # Jika word sudah pernah ditemui maka akan dilewati\n",
    "            continue\n",
    "    return(hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Separate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dan pisahkan dataset sebanyak 1000 data train dan 20 data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/corpus.tsv'\n",
    "sentences, tags = get_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengambil 1020 data awal\n",
    "data_train, data_test, tags_train, tags_test = sentences[:1000], sentences[1000:1020], tags[:1000], tags[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = list(flatten(data_train))\n",
    "tags_train = list(flatten(tags_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabel transisi dibuat berdasarkan data train yang telah diambil pada corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams, count_bigrams = get_bigram(tags_train)\n",
    "transition_table = get_prob_transition(count_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabel emisi dibuat berdasarkan data train yang telah diambil pada corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_table = get_emission_table(data_train, tags_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_common_tag(data, emmision_table):\n",
    "    # dengan menggunakan hidden table untuk membangun baseline model berdasarkan corpus\n",
    "    sentences_predict_tags = []\n",
    "    for sentence in data:\n",
    "        predict_tags = []\n",
    "        for word in sentence:\n",
    "            if word not in emmision_table.keys():\n",
    "                predict_tags.append('NN')\n",
    "            else:\n",
    "                most_common_tag = max(emmision_table[word].items(), key=operator.itemgetter(1))[0]\n",
    "                predict_tags.append(most_common_tag)\n",
    "        \n",
    "        sentences_predict_tags.append(predict_tags)\n",
    "    \n",
    "    return sentences_predict_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tags = predict_common_tag(data_test, hidden_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
